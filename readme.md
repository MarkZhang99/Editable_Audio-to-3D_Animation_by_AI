# EmoTalk_v0002: Enhanced Speech-Driven Emotional 3D Face Animation

> âš  This project is based on the original **EmoTalk** (ICCV 2023) by Ziqiao Peng et al. Please refer to the [original repository](https://github.com/psyai-net/EmoTalk_release) for baseline implementation and citation. This fork introduces architectural enhancements and fine-tuning pipelines to improve emotional expressiveness, motion precision, and controllability.

## âœ¨ What's New in EmoTalk_v0002?

EmoTalk_v0002 builds upon the original EmoTalk by introducing several key upgrades:

- âœ… **FiLM-based emotion modulation**: enables learnable fusion between emotional and phonetic features.
- âœ… **GRU-based temporal emotion encoding**: captures dynamic emotional variations across time.
- âœ… **Region-weighted blendshape loss**: enhances facial articulation accuracy, especially in lips and brows.
- âœ… **Level and identity control embeddings**: supports controllable emotional intensity and speaker style.
- âœ… **Editable blendshape output**: outputs 52D FLAME-compatible blendshape sequences for real-time animation in Blender.

## ðŸ§  Research Context

This project was conducted as an academic extension to explore:
- Emotional controllability in speech-driven animation;
- Editable facial animation pipelines compatible with Blender and FLAME;
- Trade-offs in fine-tuning with emotion-less datasets (e.g., HDTF).

## ðŸ“¦ Installation

### Environment
```bash
conda create -n emotalk_v0002 python=3.8
conda activate emotalk_v0002
pip install -r requirements.txt

### Blender Setup
wget https://ftp.nluug.nl/pub/graphics/blender/release/Blender3.4/blender-3.4.1-linux-x64.tar.xz
tar -xf blender-3.4.1-linux-x64.tar.xz
mv blender-3.4.1-linux-x64 blender

## ðŸš€ Demo

Download my pretrained checkpoint (trained on RAVDESS):

ðŸ“¦ `emotalk_finetuned.pth`  
ðŸ”— [Download link](https://livebournemouthac-my.sharepoint.com/:f:/g/personal/s5727214_bournemouth_ac_uk/EkeguJzWxFdLhgSd0LMsWuYB36y_0sAJhMnkWQcs0DA0zg?e=XXN140)

Place the file under the project root directory.

Then run:

```bash
python demo_finetuned.py \
  --wav_path "./audio/online_audio/alive.wav" \
  --result_path "./result/exp_result/7" \
  --model_path "./emotalk_finetuned.pth" \
  --level 7 \
  --person 1

python demo_finetuned.py --wav_path "./audio/online_audio/alive.wav" --result_path "./result/exp_result/7" --model_path "./emotalk_finetuned.pth" --level 7 --person 1

## Datasets
This project uses the same datasets as the original EmoTalk work (from the 3DETF dataset):
RAVDESS â€” Emotion-annotated speech audio.
HDTF â€” High-resolution talking face videos without emotion labels.
The dataset preprocessing scripts are:
prepare_ravdess.py
prepare_hdtf.py

##Acknowledgements
This project builds upon the excellent work of the original EmoTalk authors. We sincerely thank them for releasing their code and dataset.

Credits and dependencies include:
EmoTalk (ICCV 2023) â€“ Base model and design.
3DETF dataset â€“ Includes RAVDESS and HDTF data.
Wav2Vec2 pretrained models â€“ For speech representation extraction.
FLAME model & Blender integration â€“ For mesh reconstruction and visualization.
